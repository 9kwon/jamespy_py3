# Python3
# ksmart.py
# Smart learning
import tensorflow as tf
import numpy as np
from sklearn import metrics
import pandas as pd

import jinput_data

def my_model( my_w, Nx = 1000, validation_rate=0.2, test_rate=0.2):
	"""
	E.g., my_w = np.random.randn( 10)
	"""
	Ny = my_w.shape[0]
	my_x = np.random.randn( Nx, Ny)
	my_y = np.dot(my_x, my_w)
	# my_y.shape

	# reload( input_data)
	mnist = jinput_data.read_data_sets_XY( my_x, my_y, 
		validation_rate=validation_rate, test_rate=test_rate)
	return mnist

class MyModel():
	def __init__(self):
		"""
		if you generate w again using gen_w()
		your w will be different for different cases.
		"""
		# all generated data will be stored here with their corresponding w
		# self.data_l = list() 
		self.gen_w()

	def gen_w(self):
		self.my_w = np.random.randn( 10)
		return self

	def modeling( self, Nx = 1000):
		"""
		Already generated my_w is used.
		"""
		my_w = self.my_w

		Ny = my_w.shape[0]
		my_x = np.random.randn( Nx, Ny)
		my_y = np.dot(my_x, my_w)

		# self.data_l.append( [my_w, my_x, my_y])
		self.my_x, self.my_y = my_x, my_y

		return self		

	def my_model_xy( self, Nx = 1000):
		"""
		Data is generated by self.modeling() and
		only x, y values are returned. 
		"""
		self.modeling()

		return self.my_x, self.my_y

	def get_mnist_kfold( self, Nx = 1000, validation_rate=0.2, test_rate=0.2, shuffle=True):
		"""
		Data are generated and return iteration
		"""
		self.my_model_xy( Nx=Nx)
		return self.get_mnist_yield( validation_rate=validation_rate, test_rate=test_rate, shuffle=shuffle)

	def get_mnist_yield( self, validation_rate=0.2, test_rate=0.2, shuffle=True):
		"""
		Using already generated data by my_model_xy,
		iteration is performed.
		"""
		my_x, my_y = self.my_x, self.my_y

		mnist_kf = jinput_data.read_data_sets_XY_yield( my_x, my_y, 
			validation_rate=validation_rate, test_rate=test_rate, shuffle=shuffle)
		for mnist in mnist_kf:
			yield mnist

	def get_mnist( self, Nx = 1000, validation_rate=0.2, test_rate=0.2):
		my_x, my_y = self.my_model_xy( Nx = Nx)
		mnist = jinput_data.read_data_sets_XY( my_x, my_y, 
			validation_rate=validation_rate, test_rate=test_rate)
		return mnist

def multilayer_perceptron_dropout(_X, _keep_prob, _weights, _biases):
	_X_dropout = tf.nn.dropout(_X, _keep_prob)

	layer_1 = tf.nn.relu(tf.add(tf.matmul(_X_dropout, _weights['h1']), _biases['b1'])) 
	layer_1_dropout = tf.nn.dropout(layer_1, _keep_prob)

	layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1_dropout, _weights['h2']), _biases['b2'])) 
	layer_2_dropout = tf.nn.dropout(layer_2, _keep_prob)

	return tf.matmul(layer_2_dropout, _weights['out']) + _biases['out']

def open_multilayer_perceptron_dropout(_X, _keep_prob, _weights, _biases):
	"""
	All layer outputs will be shown.
	Return
	------
	layer1, layer2, out
	"""
	_X_dropout = tf.nn.dropout(_X, _keep_prob)

	layer_1 = tf.nn.relu(tf.add(tf.matmul(_X_dropout, _weights['h1']), _biases['b1'])) 
	layer_1_dropout = tf.nn.dropout(layer_1, _keep_prob)

	layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1_dropout, _weights['h2']), _biases['b2'])) 
	layer_2_dropout = tf.nn.dropout(layer_2, _keep_prob)

	out = tf.matmul(layer_2_dropout, _weights['out']) + _biases['out']
	
	return layer1, layer2, out

def tf_xy_dropout( n_input, n_classes = 1):
	"""
	dropout threshold is added.
	"""
	x = tf.placeholder("float", [None, n_input])
	y = tf.placeholder("float", [None, n_classes])
	keep_prob = tf.placeholder(tf.float32)
	return x, y, keep_prob

def tf_machine_dropout( x, y, keep_prob, n_hidden_1, n_hidden_2, learning_rate = 0.01):
	"""
	tf_machine() generates a model based on x and y
	"""
	n_input = int( x.get_shape()[1])
	n_classes = int( y.get_shape()[1])

	weights = {
		'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
		'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
	}
	biases = {
		'b1': tf.Variable(tf.random_normal([n_hidden_1])),
		'b2': tf.Variable(tf.random_normal([n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_classes]))
	}

	pred = multilayer_perceptron_dropout(x, keep_prob, weights, biases)
	cost = tf.reduce_mean( tf.square(y - pred))
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) 

	return pred, cost, optimizer

def tf_model_dropout( n_input, n_hidden_1, n_hidden_2, n_classes = 1, learning_rate = 0.01):
	"""
	Now, (x,y) and (others) can be generated separately.
	See also
	--------
	- tf_xy()
	- tf_machine()
	"""
	x, y, keep_prob = tf_xy_dropout( n_input, n_classes = n_classes)
	pred, cost, optimizer = tf_machine_dropout( x, y, keep_prob, n_hidden_1, n_hidden_2, learning_rate = learning_rate)

	return x, y, keep_prob, pred, cost, optimizer

# Create model
def multilayer_perceptron(_X, _weights, _biases):
	#Hidden layer with RELU activation
	layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights['h1']), _biases['b1'])) 
	#Hidden layer with RELU activation
	layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights['h2']), _biases['b2'])) 
	return tf.matmul(layer_2, _weights['out']) + _biases['out']

def tf_xy( n_input, n_classes = 1):
	x = tf.placeholder("float", [None, n_input])
	y = tf.placeholder("float", [None, n_classes])

	return x, y

def _tf_machine_r0( x, y, n_hidden_1, n_hidden_2, learning_rate = 0.01):
	"""
	tf_machine() generates a model based on x and y
	"""
	n_input = int( x.get_shape()[1])
	n_classes = int( y.get_shape()[1])

	weights = {
		'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
		'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
	}
	biases = {
		'b1': tf.Variable(tf.random_normal([n_hidden_1])),
		'b2': tf.Variable(tf.random_normal([n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_classes]))
	}

	pred = multilayer_perceptron(x, weights, biases)
	cost = tf.reduce_mean( tf.square(y - pred))
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) 

	return pred, cost, optimizer

def tf_machine_wb( x, y, n_input, n_hidden_1, n_hidden_2, n_classes = 1):
	n_input = int( x.get_shape()[1])
	n_classes = int( y.get_shape()[1])

	weights = {
		'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
		'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
	}
	biases = {
		'b1': tf.Variable(tf.random_normal([n_hidden_1])),
		'b2': tf.Variable(tf.random_normal([n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_classes]))
	}

	return weights, biases


def tf_machine( x, y, n_hidden_1, n_hidden_2, learning_rate = 0.01):
	"""
	tf_machine() generates a model based on x and y
	"""
	weights, biases = tf_macine_wb( x, y, n_hidden_1, n_hidden_2, n_classes = 1)
	pred = multilayer_perceptron(x, weights, biases)
	cost = tf.reduce_mean( tf.square(y - pred))
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) 

	return pred, cost, optimizer

def tf_model( n_input, n_hidden_1, n_hidden_2, n_classes = 1, learning_rate = 0.01):
	"""
	Now, (x,y) and (others) can be generated separately.
	See also
	--------
	- tf_xy()
	- tf_machine()
	"""
	x, y = tf_xy( n_input, n_classes = n_classes)
	pred, cost, optimizer = tf_machine( x, y, n_hidden_1, n_hidden_2, learning_rate = learning_rate)

	return x, y, pred, cost, optimizer


def _tf_model_r0( n_input, n_hidden_1, n_hidden_2, n_classes = 1, learning_rate = 0.01):
	weights = {
		'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),
		'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))
	}
	biases = {
		'b1': tf.Variable(tf.random_normal([n_hidden_1])),
		'b2': tf.Variable(tf.random_normal([n_hidden_2])),
		'out': tf.Variable(tf.random_normal([n_classes]))
	}

	x = tf.placeholder("float", [None, n_input])
	y = tf.placeholder("float", [None, n_classes])

	pred = multilayer_perceptron(x, weights, biases)
	cost = tf.reduce_mean( tf.square(y - pred))
	optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) 

	return x, y, pred, cost, optimizer

class DNN():
	def __init__(self, n_nodes_l, learning_rate = 0.01):
		self.learning_rate = learning_rate
		#self.training_epochs = training_epochs
		#self.batch_size = batch_size
		#self.display_step = display_step

		self.n_input = n_nodes_l[0] # MNIST data input (img shape: 28*28)
		self.n_hidden_1 = n_nodes_l[1] # 1st layer num features
		self.n_hidden_2 = n_nodes_l[2] # 2nd layer num features
		self.n_classes = 1 # for regress

		self._model() # This should be redefined. 

		init = tf.initialize_all_variables()
		self.init = init
		self.sess = tf.Session()

	def _model( self):
		n_input = self.n_input
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		n_classes = self.n_classes
		learning_rate = self.learning_rate

		x, y, pred, cost, optimizer = tf_model( n_input, n_hidden_1, n_hidden_2, 
									n_classes = n_classes, learning_rate = learning_rate)

		self.x = x
		self.y = y
		self.pred = pred
		self.cost = cost
		self.optimizer = optimizer

	def fit(self, mnist, training_epochs = 100, display_step = 10, batch_size = 10, init_flag = True):
		"""
		Inputs
		------
		init_flag: Logic
		- If it is True, variables are all initialized. 
		"""
		# Initializing the variables
		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		# training_epochs, batch_size, display_step = self.training_epochs, self.batch_size, self.display_step
		x, y = self.x, self.y
		sess = self.sess
		init = self.init

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})
				# Compute average loss
				avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch
			# Display logs per epoch step
			if epoch % display_step == 0:
				print( "Epoch:", epoch)
				print( "Train-Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

				# Test
				batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
				batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
				# Fit training using batch data
				# sess.run(optimizer, feed_dict={x: batch_xs_test, y: batch_ys_test})
				avg_cost = sess.run(cost, feed_dict={x: batch_xs_test, y: batch_ys_test})
				# Display logs per epoch step
				print( "Test:", "cost=", "{:.9f}".format(avg_cost))
				pred_test = sess.run( pred, feed_dict={x: batch_xs_test})
				r2 = metrics.r2_score( batch_ys_test, pred_test)
				print( "R2 of test data:", r2)
				
		print( 'Optimization is completed.')

		self.batch_ys_test = batch_ys_test
		self.pred_test = pred_test

		return self

	def calc_cost_r2(self, batch_xs_test, batch_ys_test_1d):
		"""
		calculate cost and r2 under the current weights
		"""
		sess = self.sess
		cost = self.cost
		pred = self.pred
		x, y = self.x, self.y

		batch_ys_test = batch_ys_test_1d.reshape( (batch_ys_test_1d.shape[0], 1))
		# Fit training using batch data
		# sess.run(optimizer, feed_dict={x: batch_xs_test, y: batch_ys_test})
		test_avg_cost = sess.run(cost, feed_dict={x: batch_xs_test, y: batch_ys_test})
		# Display logs per epoch step
		# print( "Test:", "cost=", "{:.9f}".format(test_avg_cost))
		pred_test = sess.run( pred, feed_dict={x: batch_xs_test})
		r2 = metrics.r2_score( batch_ys_test, pred_test)
		# print( "R2 of test data:", r2)

		return test_avg_cost, r2

	def traj_pd_epoch( self, traj_df_l, epoch, avg_cost, mnist, display_step, pd_step):
		if epoch % display_step == 0:
			print( "Epoch:", epoch)
			print( "Train:", "cost=", "{:.9f}".format(avg_cost))

		if epoch % pd_step == 0:
			traj_df_l.append( self.traj_pd( epoch, avg_cost, mnist))		

	def traj_pd(self, epoch, avg_cost, mnist):
		"""
		Trajectory is recorded and saved to pd dataframe.
		"""
		test_avg_cost, test_r2 = self.calc_cost_r2( mnist.test.images, mnist.test.labels)

		traj_df = pd.DataFrame()
		traj_df["epoch"] = [epoch]
		traj_df["Train-cost"] = [avg_cost]
		traj_df["Test-cost"] = [test_avg_cost]
		traj_df["Test-r2"] = [test_r2]
		return traj_df

	def fit_pd(self, mnist, training_epochs = 100, display_step = 10, pd_step = 1, batch_size = 10, init_flag = True):
		"""
		Inputs
		------
		init_flag: Logic
		- If it is True, variables are all initialized. 
		"""
		# Initializing the variables
		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		# training_epochs, batch_size, display_step = self.training_epochs, self.batch_size, self.display_step
		x, y = self.x, self.y
		sess = self.sess
		init = self.init

		traj_df_l = list()

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})
				# Compute average loss
				avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch
			
			# Display and save logs per epoch step
			self.traj_pd_epoch( traj_df_l, epoch, avg_cost, mnist, display_step, pd_step)

		print( 'Optimization is completed.')

		self.traj_df = pd.concat( traj_df_l, ignore_index = True)
		self.predict_test( mnist)

		return self

	def fit_pd_kfold(self):
		"""
		k-fold crossvalidation is appiled to split test and train. 
		"""
		print( "Underdevelopment")

	def predict_test( self, mnist):
		"""
		The prediction results will be saved as self variables. 
		"""
		sess = self.sess
		pred = self.pred
		x = self.x

		batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
		batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
		pred_test = sess.run( pred, feed_dict={x: batch_xs_test})
		
		self.batch_ys_test = batch_ys_test
		self.pred_test = pred_test

	def predict( self, mnist):
		sess = self.sess
		pred = self.pred
		x = self.x

		self.pred_train = sess.run( pred, feed_dict={x: mnist.train.images})
		self.pred_validation = sess.run( pred, feed_dict={x: mnist.validation.images})
		self.pred_test = sess.run( pred, feed_dict={x: mnist.test.images})

		self.X  = np.concatenate( [mnist.train.images, mnist.validation.images, mnist.test.images], axis = 0)
		self.y  = np.concatenate( [mnist.train.labels, mnist.validation.labels, mnist.test.labels], axis = 0) 
		self.yp = np.concatenate( [self.pred_train, self.pred_validation, self.pred_test], axis = 0)

		return self

class SDNN( DNN): # Smart DNN
	def __init__( self, n_nodes_l, learning_rate = 0.01):
		super().__init__( n_nodes_l, learning_rate = learning_rate)

	def _model( self):
		"""
		_model() should be redefined in order to extend it 
		since after _model() initialization will be performed in the __init__.
		Hence, after super().__init__(), model definition will be useless (I guess). 
		"""
		super()._model()

		n_input = self.n_input
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		n_classes = self.n_classes
		learning_rate = self.learning_rate

		# Placehold y will be reused, while placehold x will be recreated.
		y = self.y

		# new n_input_1 is created.
		# For others, we will reuse them and they can be changed later on if it is needed. 
		# Direct tensorflow calls should be welcome in order to develop very advanced deep learning algorithm
		n_input_1 = n_input + 1
		x_1 = tf.placeholder("float", [None, n_input_1])

		pred_1, cost_1, optimizer_1 = tf_machine( x_1, y, n_hidden_1, n_hidden_2, 
										learning_rate = learning_rate)

		self.n_input_1 = n_input_1
		self.pred_1 = pred_1
		self.cost_1 = cost_1
		self.optimizer_1 = optimizer_1
		self.x_1 = x_1

	def _fit_1_r0(self, mnist, training_epochs = 100, display_step = 10, batch_size = 10, init_flag = True):
		"""
		fit() for new model which use a predicted output with the previously trained model.  
		"""
		sess = self.sess
		init = self.init

		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		x, y = self.x, self.y

		pred_1, cost_1, optimizer_1 = self.pred_1, self.cost_1, self.optimizer_1
		x_1 = self.x_1

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				
				# We will predict pred (y_p) based on the previously trained model
				# Now the prediction result is used for the additional new input
				yp = sess.run( pred, feed_dict={x: batch_xs})
				batch_xs_1 = np.concatenate( [batch_xs, yp], axis = 1)
				
				sess.run(optimizer_1, feed_dict={x_1: batch_xs_1, y: batch_ys})
				# Compute average loss
				avg_cost += sess.run(cost_1, feed_dict={x_1: batch_xs_1, y: batch_ys})/total_batch
			# Display logs per epoch step
			if epoch % display_step == 0:
				print( "Epoch:", epoch)
				print( "Train-Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

				# Test
				batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
				batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
				
				yp_test = sess.run( pred, feed_dict={x: batch_xs_test})
				batch_xs_test_1 = np.concatenate( [batch_xs_test, yp_test], axis = 1)
			   
				# Fit training using batch data
				sess.run(optimizer_1, feed_dict={x_1: batch_xs_test_1, y: batch_ys_test})
				avg_cost = sess.run(cost_1, feed_dict={x_1: batch_xs_test_1, y: batch_ys_test})
				# Display logs per epoch step
				print( "Test:", "cost=", "{:.9f}".format(avg_cost))
				pred_test = sess.run( pred_1, feed_dict={x_1: batch_xs_test_1})
				r2 = metrics.r2_score( batch_ys_test, pred_test)
				print( "R2 of test data:", r2)

		print( 'x_1 Optimization is completed.')

	def fit_1(self, mnist, training_epochs = 100, display_step = 10, batch_size = 10, init_flag = True):
		"""
		fit() for new model which use a predicted output with the previously trained model.  
		"""
		sess = self.sess
		init = self.init

		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		x, y = self.x, self.y
		n_input = self.n_input # this is needed to normalized yp which will be used as an additional descriptor

		pred_1, cost_1, optimizer_1 = self.pred_1, self.cost_1, self.optimizer_1
		x_1 = self.x_1

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				
				# We will predict pred (y_p) based on the previously trained model
				# Now the prediction result is used for the additional new input
				yp = sess.run( pred, feed_dict={x: batch_xs})
				yp = yp / np.sqrt( n_input)
				batch_xs_1 = np.concatenate( [batch_xs, yp], axis = 1)
				
				sess.run(optimizer_1, feed_dict={x_1: batch_xs_1, y: batch_ys})
				# Compute average loss
				avg_cost += sess.run(cost_1, feed_dict={x_1: batch_xs_1, y: batch_ys})/total_batch
			# Display logs per epoch step
			if epoch % display_step == 0:
				print( "Epoch:", epoch)
				print( "Train-Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

				# Test
				batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
				batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
				
				yp_test = sess.run( pred, feed_dict={x: batch_xs_test})
				yp_test = yp_test / np.sqrt( n_input)
				batch_xs_test_1 = np.concatenate( [batch_xs_test, yp_test], axis = 1)
			   
				# Fit training using batch data
				# sess.run(optimizer_1, feed_dict={x_1: batch_xs_test_1, y: batch_ys_test})
				avg_cost = sess.run(cost_1, feed_dict={x_1: batch_xs_test_1, y: batch_ys_test})
				# Display logs per epoch step
				print( "Test:", "cost=", "{:.9f}".format(avg_cost))
				pred_test = sess.run( pred_1, feed_dict={x_1: batch_xs_test_1})
				r2 = metrics.r2_score( batch_ys_test, pred_test)
				print( "R2 of test data:", r2)

		print( 'x_1 Optimization is completed.')


class DNN_dropout( DNN):
	def __init__(self, n_nodes_l, learning_rate = 0.01):
		"""
		keep_prob will be defined in the fitting time.
		Moreover, 1 will be used in the prediction case.
		"""
		super().__init__(n_nodes_l, learning_rate = learning_rate)

	def _model( self):
		n_input = self.n_input
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		n_classes = self.n_classes
		learning_rate = self.learning_rate

		x, y, keep_prob, pred, cost, optimizer = tf_model_dropout( n_input, n_hidden_1, n_hidden_2, 
									n_classes = n_classes, learning_rate = learning_rate)

		self.x = x
		self.y = y
		self.keep_prob = keep_prob
		self.pred = pred
		self.cost = cost
		self.optimizer = optimizer

	def fit(self, mnist, training_epochs = 100, display_step = 10, batch_size = 10, init_flag = True, keep_prob_train = 1.0):
		"""
		Inputs
		------
		init_flag: Logic
		- If it is True, variables are all initialized. 
		"""
		# Initializing the variables
		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		# training_epochs, batch_size, display_step = self.training_epochs, self.batch_size, self.display_step
		x, y, keep_prob = self.x, self.y, self.keep_prob
		sess = self.sess
		init = self.init

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: keep_prob_train})
				# Compute average loss
				avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.0})/total_batch
			# Display logs per epoch step
			if epoch % display_step == 0:
				print( "Epoch:", epoch)
				print( "Train-Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

				# Test
				batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
				batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
				# Fit training using batch data
				# sess.run(optimizer, feed_dict={x: batch_xs_test, y: batch_ys_test})
				avg_cost = sess.run(cost, feed_dict={x: batch_xs_test, y: batch_ys_test, keep_prob: 1.0})
				# Display logs per epoch step
				print( "Test:", "cost=", "{:.9f}".format(avg_cost))
				pred_test = sess.run( pred, feed_dict={x: batch_xs_test, keep_prob: 1.0})
				r2 = metrics.r2_score( batch_ys_test, pred_test)
				print( "R2 of test data:", r2)
				
		print( 'Optimization is completed.')

		self.batch_ys_test = batch_ys_test
		self.pred_test = pred_test

		return self	


class SDNN_dropout( DNN_dropout): # Smart DNN
	def __init__( self, n_nodes_l, learning_rate = 0.01):
		"""
		Dropout is used for SDNN since yp, which is additional feature, 
		will have better prediction than others. 
		Hence, its strength should be stronger than others. 		
		"""
		super().__init__( n_nodes_l, learning_rate = learning_rate)

	def _model( self):
		"""
		_model() should be redefined in order to extend it 
		since after _model() initialization will be performed in the __init__.
		Hence, after super().__init__(), model definition will be useless (I guess). 
		"""
		super()._model()

		n_input = self.n_input
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		n_classes = self.n_classes
		learning_rate = self.learning_rate

		# Placehold y will be reused, while placehold x will be recreated.
		y = self.y
		keep_prob = self.keep_prob

		# new n_input_1 is created.
		# For others, we will reuse them and they can be changed later on if it is needed. 
		# Direct tensorflow calls should be welcome in order to develop very advanced deep learning algorithm
		n_input_1 = n_input + 1
		x_1 = tf.placeholder("float", [None, n_input_1])

		pred_1, cost_1, optimizer_1 = tf_machine_dropout( x_1, y, keep_prob, n_hidden_1, n_hidden_2, 
										learning_rate = learning_rate)

		self.n_input_1 = n_input_1
		self.pred_1 = pred_1
		self.cost_1 = cost_1
		self.optimizer_1 = optimizer_1
		self.x_1 = x_1

	def fit_1(self, mnist, training_epochs = 100, display_step = 10, batch_size = 10, init_flag = True, keep_prob_train = 0.3):
		"""
		fit() for new model which use a predicted output with the previously trained model.  
		"""
		sess = self.sess
		init = self.init

		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		x, y, keep_prob = self.x, self.y, self.keep_prob
		n_input = self.n_input # this is needed to normalized yp which will be used as an additional descriptor

		pred_1, cost_1, optimizer_1 = self.pred_1, self.cost_1, self.optimizer_1
		x_1 = self.x_1

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				
				# We will predict pred (y_p) based on the previously trained model
				# Now the prediction result is used for the additional new input
				yp = sess.run( pred, feed_dict={x: batch_xs, keep_prob: 1.0})
				yp = yp / np.sqrt( n_input)
				batch_xs_1 = np.concatenate( [batch_xs, yp], axis = 1)
				
				sess.run(optimizer_1, feed_dict={x_1: batch_xs_1, y: batch_ys, keep_prob: keep_prob_train})
				# Compute average loss
				avg_cost += sess.run(cost_1, feed_dict={x_1: batch_xs_1, y: batch_ys, keep_prob: 1.0})/total_batch
			# Display logs per epoch step
			if epoch % display_step == 0:
				print( "Epoch:", epoch)
				print( "Train-Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

				# Test
				batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
				batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
				
				yp_test = sess.run( pred, feed_dict={x: batch_xs_test, keep_prob: 1.0})
				yp_test = yp_test / np.sqrt( n_input)
				batch_xs_test_1 = np.concatenate( [batch_xs_test, yp_test], axis = 1)
			   
				# Fit training using batch data
				avg_cost = sess.run(cost_1, feed_dict={x_1: batch_xs_test_1, y: batch_ys_test, keep_prob: keep_prob_train})
				# Display logs per epoch step
				print( "Test:", "cost=", "{:.9f}".format(avg_cost))
				pred_test = sess.run( pred_1, feed_dict={x_1: batch_xs_test_1, keep_prob: 1.0})
				r2 = metrics.r2_score( batch_ys_test, pred_test)
				print( "R2 of test data:", r2)

		print( 'x_1 Optimization is completed.')

class DNN_r1():
	def __init__(self, n_nodes_l, learning_rate = 0.01):
		self.learning_rate = learning_rate
		#self.training_epochs = training_epochs
		#self.batch_size = batch_size
		#self.display_step = display_step

		self.n_input = n_nodes_l[0] # MNIST data input (img shape: 28*28)
		self.n_hidden_1 = n_nodes_l[1] # 1st layer num features
		self.n_hidden_2 = n_nodes_l[2] # 2nd layer num features
		self.n_classes = 1 # for regress

		self._model() # This should be redefined. 

		init = tf.initialize_all_variables()
		self.init = init
		self.sess = tf.Session()

	def _model_r1( self):
		"""
		Development
		-----------
		global functions and member functions will be collaborated more efficiently than now.
		_r1 is added for backup purpose.
		"""
		n_input = self.n_input
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		n_classes = self.n_classes
		learning_rate = self.learning_rate

		x, y, pred, cost, optimizer = tf_model( n_input, n_hidden_1, n_hidden_2, 
									n_classes = n_classes, learning_rate = learning_rate)

		self.x = x
		self.y = y
		self.pred = pred
		self.cost = cost
		self.optimizer = optimizer

	def tf_model_r0(self):
		"""
		Development
		-----------
		Input and output will not be usually used for a member function. 
		"""
		n_input = self.n_input
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		n_classes = self.n_classes
		learning_rate = self.learning_rate

		x, y, pred, cost, optimizer = tf_model( n_input, n_hidden_1, n_hidden_2, 
									n_classes = n_classes, learning_rate = learning_rate)

		self.x = x
		self.y = y
		self.pred = pred
		self.cost = cost
		self.optimizer = optimizer

	def tf_machine( self):
		x, y = self.x, self.y
		n_hidden_1 = self.n_hidden_1
		n_hidden_2 = self.n_hidden_2
		learning_rate = self.learning_rate

		pred, cost, optimizer = tf_machine( x, y, n_hidden_1, n_hidden_2, learning_rate = learning_rate)

		self.pred = pred
		self.cost = cost
		self.optimizer = optimizer

	def tf_xy( self):
		"""
		Using this approach which divde big function into multiple class functions. 
		"""
		n_input = self.n_input
		n_classes = self.n_classes

		x, y = tf_xy( n_input, n_classes = n_classes)

		self.x = x
		self.y = y

	def tf_model( self):
		self.tf_xy()
		self.tf_machine()

	def _model( self):
		"""
		Development
		-----------
		global functions and member functions will be collaborated more efficiently than now.
		"""
		self.tf_model()

	def fit(self, mnist, training_epochs = 100, display_step = 10, batch_size = 10, init_flag = True):
		"""
		Inputs
		------
		init_flag: Logic
		- If it is True, variables are all initialized. 
		"""
		# Initializing the variables
		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		# training_epochs, batch_size, display_step = self.training_epochs, self.batch_size, self.display_step
		x, y = self.x, self.y
		sess = self.sess
		init = self.init

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})
				# Compute average loss
				avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch
			# Display logs per epoch step
			if epoch % display_step == 0:
				print( "Epoch:", epoch)
				print( "Train-Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(avg_cost))

				# Test
				batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
				batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
				# Fit training using batch data
				# sess.run(optimizer, feed_dict={x: batch_xs_test, y: batch_ys_test})
				avg_cost = sess.run(cost, feed_dict={x: batch_xs_test, y: batch_ys_test})
				# Display logs per epoch step
				print( "Test:", "cost=", "{:.9f}".format(avg_cost))
				pred_test = sess.run( pred, feed_dict={x: batch_xs_test})
				r2 = metrics.r2_score( batch_ys_test, pred_test)
				print( "R2 of test data:", r2)
				
		print( 'Optimization is completed.')

		self.batch_ys_test = batch_ys_test
		self.pred_test = pred_test

		return self

	def calc_cost_r2(self, batch_xs_test, batch_ys_test_1d):
		"""
		calculate cost and r2 under the current weights
		"""
		sess = self.sess
		cost = self.cost
		pred = self.pred
		x, y = self.x, self.y

		batch_ys_test = batch_ys_test_1d.reshape( (batch_ys_test_1d.shape[0], 1))
		# Fit training using batch data
		# sess.run(optimizer, feed_dict={x: batch_xs_test, y: batch_ys_test})
		test_avg_cost = sess.run(cost, feed_dict={x: batch_xs_test, y: batch_ys_test})
		# Display logs per epoch step
		# print( "Test:", "cost=", "{:.9f}".format(test_avg_cost))
		pred_test = sess.run( pred, feed_dict={x: batch_xs_test})
		r2 = metrics.r2_score( batch_ys_test, pred_test)
		# print( "R2 of test data:", r2)

		return test_avg_cost, r2

	def traj_pd_epoch( self, traj_df_l, epoch, avg_cost, mnist, display_step, pd_step):
		if epoch % display_step == 0:
			print( "Epoch:", epoch)
			print( "Train:", "cost=", "{:.9f}".format(avg_cost))

		if epoch % pd_step == 0:
			traj_df_l.append( self.traj_pd( epoch, avg_cost, mnist))		

	def traj_pd(self, epoch, avg_cost, mnist):
		"""
		Trajectory is recorded and saved to pd dataframe.
		"""
		test_avg_cost, test_r2 = self.calc_cost_r2( mnist.test.images, mnist.test.labels)

		traj_df = pd.DataFrame()
		traj_df["epoch"] = [epoch]
		traj_df["Train-cost"] = [avg_cost]
		traj_df["Test-cost"] = [test_avg_cost]
		traj_df["Test-r2"] = [test_r2]
		return traj_df

	def fit_pd(self, mnist, training_epochs = 100, display_step = 10, pd_step = 1, batch_size = 10, init_flag = True):
		"""
		Inputs
		------
		init_flag: Logic
		- If it is True, variables are all initialized. 
		"""
		# Initializing the variables
		pred, cost, optimizer = self.pred, self.cost, self.optimizer
		# training_epochs, batch_size, display_step = self.training_epochs, self.batch_size, self.display_step
		x, y = self.x, self.y
		sess = self.sess
		init = self.init

		traj_df_l = list()

		if init_flag:
			sess.run(init)

		for epoch in range(training_epochs):
			avg_cost = 0.
			total_batch = int(mnist.train.num_examples/batch_size)
			# Loop over all batches
			for i in range(total_batch):
				batch_xs, batch_ys = mnist.train.next_batch(batch_size)           
				batch_ys = batch_ys.reshape( (batch_size, 1))
				# Fit training using batch data
				sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})
				# Compute average loss
				avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch
			
			# Display and save logs per epoch step
			self.traj_pd_epoch( traj_df_l, epoch, avg_cost, mnist, display_step, pd_step)

		print( 'Optimization is completed.')

		self.traj_df = pd.concat( traj_df_l, ignore_index = True)
		self.predict_test( mnist)

		return self

	def fit_pd_kfold(self):
		"""
		k-fold crossvalidation is appiled to split test and train. 
		"""
		print( "Underdevelopment")

	def predict_test( self, mnist):
		"""
		The prediction results will be saved as self variables. 
		"""
		sess = self.sess
		pred = self.pred
		x = self.x

		batch_xs_test, batch_ys_test = mnist.test.images, mnist.test.labels
		batch_ys_test = batch_ys_test.reshape( (batch_ys_test.shape[0], 1))
		pred_test = sess.run( pred, feed_dict={x: batch_xs_test})
		
		self.batch_ys_test = batch_ys_test
		self.pred_test = pred_test

	def predict( self, mnist):
		sess = self.sess
		pred = self.pred
		x = self.x

		self.pred_train = sess.run( pred, feed_dict={x: mnist.train.images})
		self.pred_validation = sess.run( pred, feed_dict={x: mnist.validation.images})
		self.pred_test = sess.run( pred, feed_dict={x: mnist.test.images})

		self.X  = np.concatenate( [mnist.train.images, mnist.validation.images, mnist.test.images], axis = 0)
		self.y  = np.concatenate( [mnist.train.labels, mnist.validation.labels, mnist.test.labels], axis = 0) 
		self.yp = np.concatenate( [self.pred_train, self.pred_validation, self.pred_test], axis = 0)

		return self


